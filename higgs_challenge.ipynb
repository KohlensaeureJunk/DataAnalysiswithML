{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example\n",
    "\n",
    "We will use the dataset from the **[Higgs Boson ML Challenge](https://www.kaggle.com/c/Higgs-boson)** since it shows a few peculiarities often encountered in particle physics applications.\n",
    "\n",
    "* The data is available from **[CERN Open Data](http://opendata.cern.ch/record/328)**.\n",
    "  * more information about the data is available from the links, and in particular in the accompanying **[documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf)**.\n",
    "  * much of the description below is taken from this documentation\n",
    "* The general idea is that we want to extract $H\\to\\tau^+\\tau^-$ signal from background. \n",
    "  * first channel where coupling of Higgs boson to fermions can be proven (before only coupling to bosons, $\\gamma$, $W$, $Z$)\n",
    "  * by now seen many other decays of Higgs, too, most recently even evidence for $H\\to\\mu^+\\mu^-$\n",
    "* In particular, selection here requires one of the taus to decay into an electron or muon and two neutrinos, and the other into hadrons and a neutrino. \n",
    "* The challenge is based on Monte Carlo collision events processed through the **[ATLAS detector](http://atlas.cern/)** simulation and reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHC and ATLAS\n",
    "* LHC collides bunches of protons every 25 nanoseconds inside ATLAS detector\n",
    "* In the hard-scattering process, two colliding protons interact and part of the kinetic energy of the protons is converted into new particles.\n",
    "* Most resulting particles are unstable and short-lived → decay quickly into a cascade of lighter particles.\n",
    "* ATLAS detector measures the properties of the decay products: type, energy and momentum (3-D direction)\n",
    "* The decay products are identified and reconstructed from the low-level analogue and digital signals they trigger in the detector hardware.\n",
    "* Part of the energy will be converted into and carried away by neutrinos (e.g. from the decay of tau leptons, $\\tau^- \\to e^- \\nu_\\tau \\bar\\nu_e$) that cannot be measured, leading to an incomplete event reconstruction and an imbalance in the total transverse momentum.\n",
    "\n",
    "Some event displays that visualize collision events found in real data that show a signature matching a $H\\to\\tau\\tau$ decay can be found on the [public ATLAS page][1]. [This event][2], for example, shows $H\\to\\tau\\tau$ with one tau lepton further decaying leptonically and the other hadronically.\n",
    "\n",
    "[1]: https://twiki.cern.ch/twiki/bin/view/AtlasPublic/EventDisplaysFromHiggsSearches#H_AN1\n",
    "[2]: https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/CONFNOTES/ATLAS-CONF-2012-160/figaux_07.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"atlas-higgs-challenge-2014-v2.csv.gz\"\n",
    "url = \"http://opendata.cern.ch/record/328/files/atlas-higgs-challenge-2014-v2.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The data contains > 800 k simulated collision events, that were used in the reference [ATLAS analysis][1]:\n",
    "* 250 k for training\n",
    "* 100 k for testing (public leaderboard)\n",
    "* 450 k for testing (private leaderboard)\n",
    "* a small withheld dataset\n",
    "\n",
    "Here, we use the full dataset:\n",
    "\n",
    "[1]: http://cds.cern.ch/record/1632191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"KaggleSet\").count()[\"EventId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset mixes background (b) and signal (s) events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Label\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the actual $s:b$ ratio were so large ($\\sim1/3$), we would have found the Higgs much earlier. \n",
    "To obtain the actual number of signal and background events we expect in the 2012 ATLAS dataset, we need to take into account the provided weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Label\").Weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, without any additional selection we expect a signal-background ratio of only 1.7 permille.\n",
    "\n",
    "Each simulated event has a weight\n",
    "* proportional to the conditional density divided by the instrumental density used by the simulator (an importance-sampling flavor),\n",
    "* and normalized for integrated luminosity (the size of the dataset; factors in cross section, beam intensity and run time of the collider)\n",
    "\n",
    "The weights are an artifact of the way the simulation works and not part of the input to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different weights correspond roughly to different background processes (due to the different cross sections)\n",
    "ax = plt.hist(df[\"Weight\"], bins = 100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three different background processes were retained in this dataset ($Z\\to\\tau\\tau$, top-quark-pair production, $W\\to\\ell\\nu$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief overview of variables, there is more information in the documentation. \n",
    "* 30 features\n",
    "  * The variables that start with **DER** are derived quantities, determined by the physicists performing the analysis as variables that discriminate signal from background. \n",
    "  * On the other hand, those that start with **PRI** are considered to be primary variables, from which the derived variables are calculated. \n",
    "    * They themselves generally do not provide much discrimination.\n",
    "    * One of the ideas suggested by deep networks is that they can determine the necessary features from the primary variables, potentially even finding variables that the physicists did not consider. \n",
    "* *EventId* identifies the event but is not a \"feature.\" \n",
    "* The *Weight* is the event weight.\n",
    "  * used to obtain the proper normalization of the different signal and background samples\n",
    "  * sum of weights of all signal events should produce the signal yield expected to be observed in 2012 LHC data taking\n",
    "  * sum of weights of all background events should produce the background yield\n",
    "* *Label* indicates if it is a signal or background event. \n",
    "* Ignore the *Kaggle* variables --- they are only used if you want to reproduce exactly what was used in the Challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate/visualize some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_columns = [col for col in df.columns if col[:3] in [\"DER\", \"PRI\"]]\n",
    "len(feat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, nrows=6, figsize=(15, 10))\n",
    "for ax, col in zip(axs.ravel(), feat_columns):\n",
    "    kwargs = dict(bins=100, histtype=\"step\", density=True)\n",
    "    ax.hist(df[col][df.Label == \"s\"], **kwargs)\n",
    "    ax.hist(df[col][df.Label == \"b\"], **kwargs)\n",
    "    ax.set_xlabel(col)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all variables are defined in each event, that's where the `-999` values are used. This happens e.g. for the leading and subleading jet and quantities derived from jets in events where there is no jet or only one jet. So it makes sense to look at the distributions of values that are not `-999`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df):\n",
    "    fig, axs = plt.subplots(ncols=5, nrows=6, figsize=(15, 10))\n",
    "    for ax, col in zip(axs.ravel(), feat_columns):\n",
    "        kwargs = dict(bins=100, histtype=\"step\", density=True)\n",
    "        x = df[col].to_numpy()\n",
    "        mask = x != -999\n",
    "        x = x[mask]\n",
    "        label = df.Label[mask].to_numpy()\n",
    "        ax.hist(x[label == \"s\"], **kwargs)\n",
    "        ax.hist(x[label == \"b\"], **kwargs)\n",
    "        ax.set_xlabel(col)\n",
    "    fig.tight_layout()\n",
    "\n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it can be instructive to look at 2D plots. The `seaborn` library provides a few helpers for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sub-set of vars for plotting\n",
    "varplot = ['DER_mass_MMC', \n",
    "           'DER_mass_jet_jet',\n",
    "           'DER_deltar_tau_lep',\n",
    "           'DER_pt_tot',\n",
    "           'PRI_jet_subleading_pt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only plot events where no variable is -999. Also we only take a random subsample of 10000 events to have reasonable scatterplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    df[~(df == -999).T.any()].sample(10000),\n",
    "    diag_kind=\"hist\",\n",
    "    vars=varplot,\n",
    "    hue=\"Label\",\n",
    "    markers=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "\n",
    "Here we have a supervised ML problem where we want to fit a model that predicts from inputs `X` some labels `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, feat_columns]\n",
    "y = df['Label']\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we will use splitting into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    weight_train,\n",
    "    weight_test,\n",
    ") = train_test_split(\n",
    "    X.to_numpy(),\n",
    "    (y == \"s\").to_numpy(),\n",
    "    weight.to_numpy(),\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ML trials w/ simple models\n",
    "1st attempt with simple models: GaussianNB and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB (Gaussian Naive Bayes, assumes each class is drawn from an axis-aligned Gaussian distribution)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_proba(p, y):\n",
    "    kwargs = dict(bins=100, range=(0, 1), histtype=\"step\")\n",
    "    plt.hist(p[y==0], label=\"background\", **kwargs)\n",
    "    plt.hist(p[y==1], label=\"signal\", **kwargs)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_proba(model.predict_proba(X_test)[:, 1], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression\n",
    "As next attempt, let's look at [logistic regression][1]. This is a very simple, linear model. In the exercises you can look at optimizing it a bit more.\n",
    "* logistic function: $f(x) = \\frac{1}{1+\\exp(-x)}$, $f(x): [-\\infty,\\infty] \\to [0,1]$\n",
    "* model: $y = f(\\vec{x} \\cdot \\vec{\\beta} + \\epsilon)$\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h3>Exercise 3</h3>\n",
    "    Likely the model showed convergence issues. Can you fix this?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_proba(model.predict_proba(X_test)[:, 1], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More sophisticated model: GradientBoostingClassifier\n",
    "The [GradientBoostingClassifier][1] provides _gradient-boosted regression trees_. \n",
    "* ensemble method that combines multiple decision trees\n",
    "* \"forward stage-wise fashion: each tree tries to correct the mistakes of the previous one (steered by the `learning_rate`)\n",
    "* trees are simple (shallow), idea is to combine many \"weak learners\" \n",
    "  * each tree can only provide good predictions on part of the data, but combined they can yield a powerful model\n",
    "  \n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's define the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=10,\n",
    "                                    min_samples_leaf=200,\n",
    "                                 subsample=0.01,\n",
    "                                    max_features=10, verbose=1)\n",
    "#gbc = HistGradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit takes several minutes... can look into AMS while it runs\n",
    "gbc.fit(X_train, y_train) # (and n_jobs is not supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prob dist\n",
    "plot_proba(df, gbc, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBC also useful to judge/quantify importance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for importance, key in reversed(sorted(zip(gbc.feature_importances_, X.keys()))):\n",
    "    print (\"%30s %6.3f\" % (key, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure-of-Merit: AMS\n",
    "Let's get back to the original problem using the GradientBoostingClassifier. The Kaggle competition used the approximate median significance ([AMS][1]), as defined below, to determine how good a solution was. \n",
    "The goal is to maximize signal and minimize background, and the AMS is an approximate formula to quantify the signal significance. The maximal AMS gives best signal significance. \n",
    "\n",
    "Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweight the inputs so that the subsample yield matches to the total yield, which we will do below.\n",
    "\n",
    "[1]: AMS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute approximate median significance (AMS)\n",
    "def ams(s,b):\n",
    "    # The number 10, added to the background yield, is a regularization term to decrease the variance of the AMS.\n",
    "    return np.sqrt(2*((s+b+10)*np.log(1+s/(b+10))-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities for an event to be label 0 (background) or 1 (signal)\n",
    "y_train_prob = gbc.predict_proba(X_train)[:, 1]\n",
    "y_test_prob  = gbc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 20% (i.e. 20 % of unweighted events above pcut will be classified as signal), but can optimize\n",
    "pcut         = np.percentile(y_train_prob, 80) # NOTE: using y_train_prob here\n",
    "sel_signal   = 100*len(y_train_prob[y_train_prob > pcut]) / len(y_train_prob)\n",
    "print(\"pcut of %f selects %.2f %% (unweighted signal events)\" % (pcut, sel_signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now include the weights** to get proper normalization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgtsig  = df[df.Label==1].Weight\n",
    "wgtback = df[df.Label==0].Weight\n",
    "\n",
    "# the density argument makes this a normalized plot (otherwise wouldn't see the signal on linear scale)\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, bins=40, density = True)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background', weights=wgtback, **kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal', weights=wgtsig, **kwargs)\n",
    "_ = plt.legend()\n",
    "\n",
    "#plt.yscale('log') -- to try without density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the total weights (yields) for all events and for training and test samples\n",
    "sigall  = weight.dot(y == 1)\n",
    "backall = weight.dot(y == 0)\n",
    "\n",
    "# training-sample weights\n",
    "sigtrain  = weight_train.dot(y_train)\n",
    "backtrain = weight_train.dot(y_train == 0)\n",
    "\n",
    "# test-sample weights\n",
    "sigtest  = weight_test.dot(y_test)\n",
    "backtest = weight_test.dot(y_test == 0)\n",
    "\n",
    "# aside:  these can also be done by looping instead of using a dot product\n",
    "#  (But usually vectorized operations are faster for interpreted code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"All  :\", sigall, backall)\n",
    "print (\"Train:\", sigtrain, backtrain)\n",
    "print (\"Test :\", sigtest, backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel  = weight_train.dot(np.multiply(y_train     , y_train_prob > pcut))\n",
    "backtrain_sel = weight_train.dot(np.multiply(y_train == 0, y_train_prob > pcut))\n",
    "\n",
    "sigtest_sel  = weight_test.dot(np.multiply(y_test     , y_test_prob > pcut))\n",
    "backtest_sel = weight_test.dot(np.multiply(y_test == 0, y_test_prob > pcut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal and background efficiency\n",
    "print (\"Train: eps_s = %f, eps_b = %f (eps_total: %f)\" % (sigtrain_sel / sigtrain, \n",
    "                                                          backtrain_sel / backtrain,\n",
    "                                                         (sigtrain_sel+backtrain_sel) / (sigtrain+backtrain)\n",
    "                                                         ))\n",
    "print (\"Test : eps_s = %f, eps_b = %f\" % (sigtest_sel / sigtest, backtest_sel / backtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to scale-up the selected yields to the (luminosity of the) original full sample\n",
    "sigtrain_sel_corr  = sigtrain_sel *sigall /sigtrain\n",
    "backtrain_sel_corr = backtrain_sel*backall/backtrain\n",
    "\n",
    "sigtest_sel_corr   = sigtest_sel *sigall /sigtest\n",
    "backtest_sel_corr  = backtest_sel*backall/backtest\n",
    "\n",
    "print(\"Scaled selected yields in training sample, signal =\", sigtrain_sel_corr, \", background =\",backtrain_sel_corr)\n",
    "print(\"Scaled selected yields in test sample, signal =\", sigtest_sel_corr, \", background =\",backtest_sel_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ROC = \"receiver operating characteristic\")\n",
    "\n",
    "An important tool to see variation of signal and background efficiency (TPR and FPR) as a function of threshold (for a selection based on predicted signal / background probabilities) at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, gbc.predict_proba(X_test)[:, 1], sample_weight = weight_test)\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "mark_threshold = pcut # mark selected threshold\n",
    "idx = np.argmin(np.abs(thresholds - mark_threshold))\n",
    "plt.plot(fpr[idx], tpr[idx], 'o', markersize=10, label=\"threshold %f\" % mark_threshold, fillstyle=\"none\", mew=2)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TPR = true positive rate = TP / (TP+FN) = TP / P (= recall)\n",
    "* FPR = false positive rate = FP / (FP+TN) = FP / N\n",
    "\n",
    "_Note_: As we have a lot more background than signal events, we typically want to choose a point with a very low false-positive rate. While we can use the ROC curve to compare different classifiers, a better performance measure is the AMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_corr, backtrain_sel_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_corr, backtest_sel_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create plot of AMS vs Pcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ams_vals = ams(tpr*sigall, fpr*backall)\n",
    "print(\"Maximum AMS {:.3f} for pcut {:.3f}\".format(ams_vals.max(), thresholds[np.argmax(ams_vals)]))\n",
    "plt.plot(thresholds, ams_vals)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut') # x-axis\n",
    "plt.ylabel('AMS')# y-axis\n",
    "# mark chosen pcut\n",
    "idx = np.argmin(np.abs(thresholds - mark_threshold))\n",
    "plt.plot(thresholds[idx], ams_vals[idx], 'o', markersize=10, label=\"threshold %f\" % mark_threshold, fillstyle=\"none\", mew=2)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Not too bad. Here are the scores of real submissions.\n",
    "![Comparison with submissions](figures/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course a bit of a simplification from a real physics analysis, where systematics often seem to take the most time. They are ignored here.\n",
    "![Comparison with real analysis](figures/tr140415_davidRousseau_Rome_Higgs_MVA_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _systematics_: systematic uncertainties on the event yields and BDT distributions, of experimental and theoretical origin (cf. section 11 in reference analysis)\n",
    "* _categories_: the reference analysis discriminates two production mechanisms of the Higgs boson, VBF (events with two characteristic jets from vector-boson fusion) and boosted (all other events, dominated by gluon fusion)\n",
    "* _embedded_: dominant Z→τ⁺τ⁻ background is taken from \"τ-embedded Z→μ⁺μ⁻ data\"\n",
    "* _anti tau_: revert some tau-identification criterion to create an \"anti(-ID) tau\" sample (used in \"fake-factor method\" to estimate background with objects misidentified as tau leptons)\n",
    "* _control regions_: phase-space regions enriched in (one type of) background process that allow to normalize a predicted background contribution to that observed in data\n",
    "* _tt_: background process, events with pair production of top quarks ($t\\bar t$)\n",
    "* _NP_: nuisance parameters (parameters of fit model that are not of physical interest but give it more flexibility to describe the data)\n",
    "* _TMVA_: [Toolkit for Multivariate Data Analysis with ROOT][1], a ROOT-integrated project providing an ML environment for multivariate classification and regression techniques\n",
    "\n",
    "[1]: https://root.cern.ch/tmva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Your tasks\n",
    "What to work on for the rest of the day:\n",
    "1. Look at the definition of the variables in the manual to get a rough feeling for what physics they encode.\n",
    "1. Attempt to calculate the AMS for the logistic regression cases.\n",
    "1. Do we overfit? Add plots to see.\n",
    "1. Which variables are important?\n",
    "1. Should we **[preprocess](http://scikit-learn.org/stable/modules/preprocessing.html)** the input data to be the same scale? Note that we have some -999 values that indicate the variable could not be calculated.\n",
    "We will discuss how to treat such problems of the input data in the [next notebook][1].\n",
    "1. We do not use the event weights in the training. Can they help? Note, that you don't want to just apply the weights as is since they will make background dominate over signal.\n",
    "1. The best scores in the Challenge all used cross-validation; if you have time, try to implement it.\n",
    "\n",
    "*Later we will [continue][2] on this example with neural networks.*\n",
    "\n",
    "[1]: Higgs-Gaussian.ipynb\n",
    "[2]: HiggsChallenge-NN.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
