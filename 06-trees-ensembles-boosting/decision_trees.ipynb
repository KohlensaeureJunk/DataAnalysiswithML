{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree and Random Forest Models\n",
    "\n",
    "based on based on [DecisionTrees.ipynb from LMU block course](https://github.com/fuenfundachtzig/LMU_DA_ML_Basic/blob/main/notebooks/DecisionTrees.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "A further important model category. The basic principle is easy to understand:  \n",
    " Hierarchical series of  **if/else questions** \n",
    "\n",
    "*Example:* Game where you need to distinguish four kinds of animals:  \n",
    "* *Bear, Dolphin, Penguin, Hawk*\n",
    "\n",
    "Goal is to use as few questions as possible.\n",
    "\n",
    "One possible solution:\n",
    "\n",
    "![](figures/DT_animals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple example \n",
    "Illustrate DT with half-moon data, a simple dataset with half-moon shaped data distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(*X[y==0].T, color=\"red\")\n",
    "plt.scatter(*X[y==1].T, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try previous models first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3).fit(X, y)\n",
    "tree.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_classifier(predict, xmin, xmax, ymin, ymax, **kwargs):\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(xmin, xmax, 100),\n",
    "        np.linspace(ymin, ymax, 100),\n",
    "    )\n",
    "    X = np.stack([xx, yy], axis=-1).reshape(-1, 2)\n",
    "    zz = predict(X).reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, zz, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization with different depths8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_depth = 1\n",
    "model = DecisionTreeClassifier(max_depth=max_depth).fit(X, y)\n",
    "print(f\"{max_depth=}, {model.score(X, y)=}\")\n",
    "visualize_classifier(model.predict, -1.5, 2.5, -1.5, 2, cmap=\"RdBu\")\n",
    "plt.scatter(*X[y==0].T, color=\"red\")\n",
    "plt.scatter(*X[y==1].T, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high depth, clearly goes into over-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree example with real data\n",
    "\n",
    "A frequently used data set for ML is a data set for *breast cancer diagnosis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "print (cancer.feature_names)\n",
    "print (cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply decision-tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=42\n",
    ")\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without limiting the depth, the DT will be evolved until perfect accuracy.\n",
    "\n",
    "But not really useful &rarr; Over-training\n",
    "\n",
    "Better approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the performance on the test set has improved by introducing a maximum depth of the trees. (The fact that we do no longer get perfect classfication on the training sample is not relevant.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Decisions trees are potentially very powerful models but they are very also sensitive to overtraining (overfitting); therefore they are normally not directly used in practice. \n",
    "\n",
    "However, one can mitigate or solve this problem by using an ensemble of decision trees and not just a single DT.  \n",
    "The main trick is randomization:\n",
    "* train many DTs but\n",
    "    * each DT sees different parts of the data\n",
    "    * or different set of features\n",
    "\n",
    "This approach is called **Random Forest**:  \n",
    "Many randomized trees contribute and the final decision is made by some sort of majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with half moon data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_classifier(lambda X: forest.predict_proba(X)[:, 1], -1.5, 2.5, -1.5, 2.5, cmap=\"RdBu\")\n",
    "plt.scatter(*X[y==0].T, color=\"red\")\n",
    "plt.scatter(*X[y==1].T, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest for Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0\n",
    ")\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-box already better accuracy on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "A very useful additional result of DT classification is the *feature importance*.\n",
    "This gives for each feature a rating between 0 and 1 how important it is for the classification:\n",
    "* 0 means no effect, not useful\n",
    "* 1 means perfect separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(forest.feature_importances_, index=cancer.feature_names).sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0\n",
    ")\n",
    "bdt = GradientBoostingClassifier(n_estimators=1000, max_depth=3, learning_rate=0.01, verbose=True, subsample=0.5, max_features=\"sqrt\")\n",
    "bdt.fit(X_train, y_train)\n",
    "\n",
    "bdt.score(X_train, y_train), bdt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(bdt.feature_importances_, index=cancer.feature_names).sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "There is a nice interactive tool that helps to understand how decision trees work:\n",
    "\n",
    "[![Screenshot](figures/screenshot_BDT_playground.png)](https://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)\n",
    "\n",
    "This also allows to use rotated decision trees, originally proposed in [2006](https://ieeexplore.ieee.org/document/1677518). You can read more about this e.g. [here](https://jmlr.csail.mit.edu/papers/volume17/blaser16a/blaser16a.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
