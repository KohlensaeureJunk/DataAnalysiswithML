{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "Based on [DataRescaling.ipynb.ipynb from LMU course](https://github.com/fuenfundachtzig/LMU_DA_ML_Basic/blob/main/notebooks/DataRescaling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features in our data are just numbers without units attached to them. However, giving a length in cm or inch (or in [Newton seconds vs pound-force seconds][1]) will give different values and thus different outputs when feeding these numbers e.g. to a neural network. \n",
    "\n",
    "---\n",
    "### Mars Climate Orbiter: [a sad story](https://en.wikipedia.org/wiki/Mars_Climate_Orbiter#Cause_of_failure)\n",
    "* robotic space probe, mission launched Dec 98, planned to reach Mars orbit in Sep 99\n",
    "* Trajectory Correction Maneuver-4 was computed beginning of September\n",
    "    * some ground software by Lockheed Martin produced results in a United States customary unit, contrary to its specification, while a second system by NASA expected those results to be in SI units\n",
    "    * specifically: the total impulse produced by thruster firings was given in pound-force seconds instead of Ns (factor 4.45 mismatch)\n",
    "* unfortunately, the disagreement was noticed but the concern by the engineers ignored because they didn't fill the forms correctly\n",
    "    * a 5th correction maneuver was considered but not done\n",
    "* the probe entered a too low Mars orbit (57 km instead of 226 km)\n",
    "    * communication was lost on 23.09.1999 when it went into orbital insertion\n",
    "    * likely skipped from the atmosphere and reentered space or was destroyed\n",
    "* total cost of the mission: around 0.33 billion USD\n",
    "\n",
    "---\n",
    "\n",
    "Even more importantly, the values of different features may naturally cover completely different ranges. This may have a large effect on the performance of machine learning models:\n",
    "* obviously e.g. when computing the distance of nearest neighbors in the kNN estimator based on several features\n",
    "* but also if a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly.\n",
    "* rescaling features to have mean 0 and standard deviation 1 effectively gets rid of the units: instead, we measure in “standard deviations from the mean”\n",
    "\n",
    "In general, learning algorithms benefit from standardization of the data set.\n",
    "Some algorithms will be more susceptible to the scaling of the data than others:\n",
    "* Neural networks expect all input features to vary in a similar way, and ideally to look like standard normally distributed data, i.e. Gaussian with zero mean and unit variance.\n",
    "* Decision Trees and BDTs just apply `if` statements on the features and are typically very robust.\n",
    "\n",
    "The `sklearn` documentation provides further information on [preprocessing of data][2].\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Mars_Climate_Orbiter#Cause_of_failure\n",
    "[2]: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    Read about different <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#original-data\">scalers in <code>sklearn</code></a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example I\n",
    "One example that illustrates the importance of preprocessing the data to rescale feature values to \"standard ranges\" is the application of a *support vector machine* (SVM) to `sklearn`'s cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear support vector classifiers (SVC) try to find a (hyper-) plane in phase space that optimally separates the two given populations. \n",
    "* Predictions are then made by checking on which side of this plane new samples lie. \n",
    "* The (hyper-) plane is defined by only a few of the training samples (the supporting vectors of features); those that lie close to the resulting plane.\n",
    "* note: distinction of *generative* classification vs *discriminative* classification: \n",
    "  * generative classification: model each class and compute likelihood of belonging to each (e.g. Gaussian Naive Bayes)\n",
    "  * discriminative classification: find curve or manifold in feature space separating the classes from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here a SVM implemented in `sklearn.svm.SVC`. This is (by default) not a linear SVC but a kernelized SVC which can yield a more complex model than a simple hyperplanes in the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset have very different ranges as illustrated by the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.boxplot(cancer.data, showfliers=False) # boxes: lower to upper quartile, whiskers: Q1-1.5IQR/Q3+1.5IQR, outliers are not shown\n",
    "plt.xlabel(\"feature\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data without rescaling\n",
    "svm = SVC(gamma=\"auto\")\n",
    "svm.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very low accuracy compared to our earlier results.\n",
    "\n",
    "(Note that `SVC` by default uses `kernel=\"rbf\"`, i.e. a [radial-basis-function kernel][1]. The use of the Euclidean distance between the feature vectors may be causing this. If we instead used a linear SVC (`SVC(kernel=\"linear\")`), the resulting score would be much higher.)\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Radial_basis_function_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now rescale the features and retrain\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# compute minimum and maximum on the training data\n",
    "# note that we must train the scaler only on the training data (but not the full dataset)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# rescale the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "# retrain\n",
    "svm.fit(X_train_scaled, y_train).score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "plt.boxplot(scaler.transform(cancer.data), showfliers = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same effect (although much smaller) e.g. on MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "print(\"Score using unscaled data:\", mlp.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"Score using rescaled data:\", mlp.fit(X_train_scaled, y_train).score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no effect e.g. on BDT\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "bdt = GradientBoostingClassifier(random_state=0)\n",
    "print(\"Score using unscaled data:\", bdt.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"Score using rescaled data:\", bdt.fit(X_train_scaled, y_train).score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large values of features may also lead to convergence issues. \n",
    "(Note that many linear models include regularization terms that are introduced to avoid very large values of the parameters.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a quick function to define a dataset with two populations that should be easy to separate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def make_line(a, b, noise, nsamples):\n",
    "    x = np.random.rand(nsamples)\n",
    "    return np.stack([x, a * x + b + (np.random.rand(nsamples) - 0.5) * noise], axis=1)\n",
    "\n",
    "def make_xy_corr_sample(noise=0.3, nsamples=50, xscale=1, yscale=1e5):\n",
    "    X = np.concatenate(\n",
    "        [\n",
    "            make_line(0.5, 0, noise=noise, nsamples=nsamples),\n",
    "            make_line(0.5, 0.5, noise=noise, nsamples=nsamples),\n",
    "        ],\n",
    "    )\n",
    "    X[:, 0] *= xscale\n",
    "    X[:, 1] *= yscale\n",
    "    y = np.concatenate([np.zeros(nsamples), np.ones(nsamples)])\n",
    "    return X, y\n",
    "\n",
    "X, y = make_xy_corr_sample()\n",
    "plt.scatter(*X[y==0].T)\n",
    "plt.scatter(*X[y==1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot: We have two populations that are obviously easy to separate? Or are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two populations should be easy to separate with a line, so we will try a linear SVC in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_xy_corr_sample()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "### fit\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC().fit(X_train, y_train)\n",
    "\n",
    "print(\"Score on training data:\", model.score(X_train, y_train))\n",
    "print(\"Score on validation data:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training data shows that we cannot even model the training data correctly using the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_classifier(model, xmin, xmax, ymin, ymax, **kwargs):\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(xmin, xmax, 100),\n",
    "        np.linspace(ymin, ymax, 100),\n",
    "    )\n",
    "    X = np.stack([xx, yy], axis=-1).reshape(-1, 2)\n",
    "    zz = model.predict(X).reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, zz, **kwargs)\n",
    "\n",
    "\n",
    "vis_classifier(\n",
    "    model, X[:, 0].min(), X[:, 0].max(), X[:, 1].min(), X[:, 1].max(), cmap=\"RdBu_r\", alpha=0.8\n",
    ")\n",
    "plt.scatter(*X[y == 0].T, color=\"blue\")\n",
    "plt.scatter(*X[y == 1].T, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h3>Exercise 2</h3>\n",
    "    How can we solve this problem?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
