{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset\n",
    "\n",
    "We take the  **[Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)** as example. This had already been introduced by famous statistician\n",
    "Ronald Fisher in 1936 and is used since then as instructive use case for classification. \n",
    "The data consists of\n",
    "* measurements of length and width of both sepal (Bl&uuml;tenkelch) and petal (Bl&uuml;te) \n",
    "* classification of Iris sub-species\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaboorn provides easy way to import iris dataset as pandas dataframe\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "First step should always be some investigation of data properties, i.e.\n",
    "* basic statistical properties\n",
    "* visualization of distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics with pandas\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of single feature\n",
    "sns.histplot(data=iris,x='sepal_length',hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined plot of 2 features\n",
    "sns.jointplot(data=iris,x='sepal_length',y='sepal_width', hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined plot matrix of all features in dataframe\n",
    "#\n",
    "# will provide scatter plot of all combinations of numerical columns in dataframe\n",
    "# target (=species) can be given and will cause different colors\n",
    "sns.pairplot(iris, hue='species', diag_kind='hist', height=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Dimensionality Reduction \n",
    "The Iris data is also a good show case  **dimensionality reduction**, i.e. check if there is a lower dimensional representation which retains the essential features.\n",
    "* In case of Iris data there are four feature dimensions\n",
    "* scatter plot showed clear correlations between features\n",
    "  * indication that less dimensions might be sufficient\n",
    "  \n",
    "One standard method is principal component analysis (PCA), which can be applied in case of (reasonably) linear correlations.\n",
    "\n",
    "As before we have to do the usual scikit steps:\n",
    "* Setup PCA model \n",
    "* fit/train\n",
    "* get reduced dimensions as output of transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in again iris dataset and store feature matrix\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "# feature matrix\n",
    "X=iris.loc[:,'sepal_length':'petal_width']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional scaling**\n",
    "\n",
    "Next codebox does scaling of features to common mean and spread, this can be important for PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn = X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "scaler.fit(X)\n",
    "# uncomment next line to get effect\n",
    "#Xn = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup and fit model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  # 1. Choose the model class\n",
    "#model = PCA(n_components=2)            # 2. Instantiate the model, fix to 2 parameters\n",
    "model = PCA()                         # 2. Instantiate -- don't restrict # params\n",
    "#model = PCA(n_components=0.9)         # 2. Instantiate -- n-parameters until 90% of variance isreached \n",
    "model.fit(Xn)                           # 3. Fit to data. Notice y is not specified!\n",
    "X_2D = model.transform(Xn)              # 4. Transform the data to two dimensions\n",
    "X_2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize transformed data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", hue='species', data=iris, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the coefficients of the PCA transformation using the `model.components_` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(model.components_)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X.columns)), X.columns)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we plot the correlation like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"PCA1\", y=\"petal_width\", hue='species', data=iris, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model provides information on covered variance per par**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(model.explained_variance_ratio_))+1,model.explained_variance_ratio_)\n",
    "plt.title('PCA explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.explained_variance_ratio_*100,np.cumsum(model.explained_variance_ratio_*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "* Redo PCA\n",
    "  * no constraint on n_components --> 4 pars\n",
    "  * n_components = 0.95 --> model uses as many componentes as needed to obtain 95% variance coverage\n",
    "  * repeat with scaled X features\n",
    "  *\n",
    "  \n",
    "* Do some basic classification (eg kNN, logistic regression) using the 2 PCA Iris components and compare with the original kNN using all 4 Iris features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Of course we can also try our clustering methods on the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means model\n",
    "A very simple model is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture model\n",
    "One powerful method is Gaussian mixture model (GMM) *(Details see Data Science Handbook: 05.12-Gaussian-Mixtures.ipynb)*  \n",
    "A GMM attempts to model the data as a collection of Gaussian blobs.\n",
    "\n",
    "We can fit the Gaussian mixture model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture       # 1. Choose the model class\n",
    "#\n",
    "model =  GaussianMixture(n_components=3,\n",
    "                         covariance_type='full')  # 2. Instantiate the model with hyperparameters\n",
    "\n",
    "model.fit(X)                                      # 3. Fit to data. Notice y is not specified!\n",
    "y_gmm = model.predict(X)                          # 4. Determine cluster labels\n",
    "#model.fit(X_scaled)                                      # 3. Fit to data. Notice y is not specified!\n",
    "#y_gmm = model.predict(X_scaled)                          # 4. Determine cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['cluster'] = y_gmm\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", data=iris, hue='species',\n",
    "           col='cluster', fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot PCA data for each identified cluster  \n",
    "Indicates good clustering, basically identical to species.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# PCA applied to digit data\n",
    "\n",
    "Another classic example case for ML is handwritten digits data.\n",
    "\n",
    "A suitable dataset is included with sklearn, first we look into it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is sklearn specific container, basically a list of 8x8 pixels images\n",
    "\n",
    "We plot a sub-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot shows pixel image together with label (in green).\n",
    "\n",
    "* Some images are obvious\n",
    "* Others seem difficult "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at data from 1st image --> 2D table resembles 0\n",
    "print (digits.images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data with sklearn:\n",
    "To use the data with sklearn as before we need a 2D structure: `samples x features` , i.e. the 8x8 images should be transformed into flat 1x64 array.   \n",
    "\n",
    "Already provided in Dataset, element `data` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (digits.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use as before just re-label\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try PCA\n",
    "from sklearn.decomposition import PCA  # 1. Choose the model class\n",
    "#model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters -- 2 pars\n",
    "model = PCA(n_components=0.9)         # 2. Instantiate the model with hyperparameters -- # pars up to 90% coverage\n",
    "model.fit(X)                           # 3. Fit to data. Notice y is not specified!\n",
    "X_2D = model.transform(X)              # 4. Transform the data to two dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Check covered variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.explained_variance_ratio_*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(model.explained_variance_ratio_*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.bar(np.arange(len(model.explained_variance_ratio_))+1,model.explained_variance_ratio_)\n",
    "plt.bar(np.arange(len(model.explained_variance_ratio_))+1,np.cumsum(model.explained_variance_ratio_))\n",
    "plt.title('PCA explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**now reduce 64 to 2 dimensions and visualize it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.decomposition import PCA  # 1. Choose the model class\n",
    "model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters -- 2 pars\n",
    "model.fit(X)                           # 3. Fit to data. Notice y is not specified!\n",
    "X_2D = model.transform(X)              # 4. Transform the data to two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout=pd.DataFrame()\n",
    "xout['tag']=y\n",
    "xout['PCA1'] = X_2D[:, 0]\n",
    "xout['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", hue='tag', data=xout, fit_reg=False, markers='.');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some digits are nicely isolated, others less so\n",
    "\n",
    "Think about it, which digits tend to look similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the *principle components* that the PCA has extracted from the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PCA components of digits\n",
    "fig, ax = plt.subplots(1, 2, subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for idx, comp in enumerate(model.components_[:2]):\n",
    "    img = comp.reshape(8,8)\n",
    "    ax.ravel()[idx].imshow(img, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left image shows the most important, the right image the second-most important component extracted by the PCA.\n",
    "Compare this to the previous plot to see that this actually makes sense: \n",
    "* If you focus on the blue (\"negative\") pixels in the left image, those resemble the digit \"3\". Indeed, from the previous plot we see that the figures 3 cluster at low values of PCA1 (and around 0 for PCA2, i.e. they typically have not much of the second component in the right image). \n",
    "* The red in the left image could be part of a \"4\" which indeed has high values for PCA1.\n",
    "* Similarly, the red in the right image is somewhat the outline of a \"0\" which has large positive values for PCA2 (and small absolute values for PCA1).\n",
    "\n",
    "Can you find the digit \"1\"?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
