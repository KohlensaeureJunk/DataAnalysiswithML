{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example using Neural Networks\n",
    "In this part we continue to work with the data from the **[Higgs Boson ML Challenge][1]** on Kaggle and present solutions using neural networks (NN). \n",
    "\n",
    "It is based on [HiggsChallenge-NN.ipynb.ipynb from LMU course][2]\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://www.kaggle.com/c/Higgs-boson\n",
    "[2]: https://github.com/fuenfundachtzig/LMU_DA_ML/blob/master/HiggsChallenge-NN.ipynb\n",
    "[3]: NN_Activation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks to discover the Higgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start trying to apply a NN to the Higgs Challenge data. We will start using Scikit Learn, and then try **[Keras](https://keras.io/)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path(\"atlas-higgs-challenge-2014-v2.csv.gz\")\n",
    "\n",
    "def prepare_data(path):\n",
    "    if path.exists():\n",
    "        return\n",
    "    url = \"http://opendata.cern.ch/record/328/files/atlas-higgs-challenge-2014-v2.csv.gz\"\n",
    "    path_prev_tutorial = Path(\"../05-validation-and-metrics\") / path\n",
    "    if path_prev_tutorial.exists():\n",
    "        path.symlink_to(path_prev_tutorial)\n",
    "    if not path.exists():\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "\n",
    "prepare_data(path)\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sig_tot = df[\"Weight\"][df.Label == \"s\"].sum()\n",
    "n_bkg_tot = df[\"Weight\"][df.Label == \"b\"].sum()\n",
    "# comment this out if you want to run on the full dataset\n",
    "df = df.sample(frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "columns = list(X.columns)\n",
    "X = X.to_numpy()\n",
    "y = df['Label'].to_numpy()\n",
    "weight = df['Weight'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks (**M**ulti**L**ayer **P**erceptrons - MLP) in sklearn\n",
    "\n",
    "Let's first look at the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(verbose=True, early_stopping=True, max_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the [approximate median significance][1] from the Kaggle competition to determine how good a solution was. Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the total yield, which we will do below.\n",
    "\n",
    "[1]: AMS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load function to compute approximate median significance (AMS)\n",
    "from mltools import ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine probability scores\n",
    "y_train_prob = mlp.predict_proba(X_train)[:, 1]\n",
    "y_test_prob = mlp.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add the probability to the original data frame\n",
    "df['Prob']=mlp.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mltools import plot_proba\n",
    "plot_proba(df, mlp, X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the total weights (yields)\n",
    "#sigall  = weight[y==1].sum()\n",
    "#backall = weight[y==0].sum()\n",
    "# need to use numbers for full sample\n",
    "sigall,backall = n_sig_tot, n_bkg_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mltools import ams_scan\n",
    "label='Train'\n",
    "pcutv,amsv = ams_scan(y_train, y_train_prob, weight_train, sigall, backall)\n",
    "\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "label='Test'\n",
    "pcutv,amsv = ams_scan(y_test, y_test_prob, weight_test, sigall, backall)\n",
    "\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Worse than the BDT from \n",
    "[higgs_challenge.ipynb](../05-validation-and-metrics/higgs_challenge.ipynb)\n",
    "\n",
    "![Comparison with submissions](figures/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling\n",
    "Neural networks are quite sensitive to feature scaling, so let's try to scale the features.\n",
    "\n",
    "And set missing values to 0 before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train[X_train == -999] = 0\n",
    "X_test[X_test == -999] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(X_train[:, columns.index(\"DER_mass_MMC\")], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(X_train_scaled[:, columns.index(\"DER_mass_MMC\")], bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new network using the rescaled features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_scaled = MLPClassifier(verbose=True, early_stopping=True, max_iter=40)\n",
    "mlp_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_scaled.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_scaled.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine probability scores\n",
    "y_train_prob_scaled = mlp_scaled.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_prob_scaled = mlp_scaled.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mltools import ams_scan\n",
    "label='Train'\n",
    "pcutv,amsv = ams_scan(y_train, y_train_prob_scaled, weight_train, sigall, backall)\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "label='Test'\n",
    "pcutv,amsv = ams_scan(y_test, y_test_prob_scaled, weight_test, sigall, backall)\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved quite a bit by using the same classifier but with rescaled data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Exercise 1</h2>\n",
    "    Check documentation of the MLPClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) and vary the structure of the network (number of hidden layers, number of neurons)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral networks with Keras\n",
    "SciKit Learn has simple NNs, but if you want to do deep NNs, or train on GPUs, you probably want to use something like [Keras](https://keras.io/getting_started/) instead. \n",
    "\n",
    "Let's try to create a simple NN, similar to the one sklearn gave us using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=100, activation=\"relu\", input_shape=X_train.shape[1:], kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    Dense(units=1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Dense`: \"Just your regular densely-connected NN layer.\"\n",
    "  * implements the operation: output = activation(dot(input, kernel) + bias)\n",
    "    * kernel is a weights matrix created by the layer\n",
    "    * bias is a bias vector created by the layer (only applicable if `use_bias` is True)\n",
    "  * `units`: dimensionality of the output array (note: we do not need to specify to size of the input array, except...)\n",
    "  * `input_shape`: expected shape of the input arrays (...only needed for the first layer)\n",
    "  * `activation`: element-wise activation function\n",
    "  * `kernel_regularizer`: constraint function applied to the kernel weights matrix (see [regularizers][2])\n",
    "  \n",
    "  \n",
    "[1]: https://keras.io/constraints/\n",
    "[2]: https://keras.io/api/layers/regularizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `optimizer`: name of optimizer or optimizer instance. See [optimizers][1].\n",
    "  * _Adam_: an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments ([paper][2], a short [summary][4])\n",
    "* `loss`: name of objective function or objective function. See [losses][3].\n",
    "  * _binary crossentropy_: \n",
    "    $$H_p(q) = -\\frac{1}{N}\\sum_{i=1}^N [{y_i} \\log(\\hat{y}_i)+(1-y_i) \\log(1-\\hat{y}_i)]$$\n",
    "    * a measure of dissimilarity, used here to define the loss function that should be minimized: \n",
    "    \n",
    "        \"The cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\"\n",
    "        \n",
    "        (The minimum number of bits to encode an independent event that occurs with probability $y_i$ is $-\\log_2(y)$.)\n",
    "   * here the true labels are $y_i=1$ for the positive class and $y_i=0$ for the negative class\n",
    "   * the estimated probabilities are $\\hat y_{i}$\n",
    "   * $N$ runs over all samples\n",
    "* `metrics`: list of metrics to be evaluated by the model during training and testing (typically accuracy)\n",
    "\n",
    "[1]: https://keras.io/optimizers/\n",
    "[2]: https://arxiv.org/abs/1412.6980v8\n",
    "[3]: https://keras.io/losses/\n",
    "[4]: https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218\n",
    "[5]: https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=200, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `batch_size`: number of samples per gradient update\n",
    "* `epochs`: number of epochs to train the model. An epoch is an iteration over the entire training dataset provided. \n",
    "\n",
    "[Further discussion...](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize training history returned by model.fit\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.predict` method will give us the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_prob_keras = model.predict(X_train_scaled)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can treat keras models like functions (note this will return tensorflow tensors which you might want to convert to numpy). When data fits into memory this is often fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_prob_keras = model(X_train_scaled).numpy()\n",
    "y_test_prob_keras = model(X_test_scaled).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mltools import ams_scan\n",
    "label='Train'\n",
    "pcutv,amsv = ams_scan(y_train, y_train_prob_keras, weight_train, sigall, backall)\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "label='Test'\n",
    "pcutv,amsv = ams_scan(y_test, y_test_prob_keras, weight_test, sigall, backall)\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only made a single layer NN in Keras. However, you can easily change the structure of the network. As an assignment, try adding an extra hidden layer and changing the number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Exercise 2</h2>\n",
    "    We only made a single layer NN in Keras. However, you can easily change the structure of the network. Try adding an extra hidden layer and changing the number of neurons.\n",
    "</div>    \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h3>Further variations:</h3>\n",
    "    - Vary the activation.\n",
    "    - Vary the regularization. May have to do this as the structure changes.\n",
    "    - Try using derivied variables only or primary variables only.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
